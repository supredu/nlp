import torch
from transformers import AutoTokenizer
from model.model import MiniMindLM
from model.config import LMConfig

# === 1. æ¨¡å‹é…ç½® ===
config = LMConfig(
    dim=512,
    n_layers=8,
    n_heads=8,
    n_kv_heads=2,
    vocab_size=6400,  # ç¡®ä¿ä¸ tokenizer ä¸€è‡´
    model_max_length=512,
)

# === 2. åŠ è½½ tokenizer ===
tokenizer = AutoTokenizer.from_pretrained("model/tokenizer")

# === 3. åŠ è½½æ¨¡å‹æƒé‡ ===
model = MiniMindLM(config)
checkpoint = torch.load("out/sft_512.pth", map_location="cpu")
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

# === 4. å‡†å¤‡è¾“å…¥å¹¶ç”Ÿæˆç¿»è¯‘ ===
prompt = "Translate to Traditional Chinese: The stock market crashed today due to unexpected inflation data."
inputs = tokenizer(prompt, return_tensors="pt").to("cpu")

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.7,
        top_p=0.9
    )

# === 5. æ‰“å°è¾“å‡º ===
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nğŸ§ª Prompt:", prompt)
print("ğŸ“˜ Output:", result)
